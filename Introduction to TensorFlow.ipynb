{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1184e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b161b",
   "metadata": {},
   "source": [
    "#### Linear function in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295a786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function():\n",
    "    \n",
    "    X = tf.Variable(np.random.randn(3,1), name = 'X')\n",
    "    W = tf.Variable(np.random.randn(4,3), name = 'W')\n",
    "    b = tf.Variable(np.random.randn(4,1), name = 'b')\n",
    "    Y = tf.add(tf.matmul(W, X), b)\n",
    "    \n",
    "    return Yata.Dataset.from_tensor_slices(train_dataset['train_set_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4e432",
   "metadata": {},
   "source": [
    "#### Computing Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322790df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "# tf.keras.activations.sigmoid requires float16, float32, float64, complex64, or complex128.\n",
    "    \n",
    "    z = tf.cast(z, tf.float32)\n",
    "    a = tf.keras.activations.sigmoid(z)\n",
    "    \n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce5570",
   "metadata": {},
   "source": [
    "#### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7b889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(label, depth=6):\n",
    "    one_hot = tf.reshape(tf.one_hot(label, depth, axis=0),shape=[-1, ])\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb34a5",
   "metadata": {},
   "source": [
    "#### Initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89255bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():    \n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed=1)   \n",
    "   \n",
    "    W1 = tf.Variable(initializer(shape=[25, 12288]))\n",
    "    b1 = tf.Variable(initializer(shape=[25, 1]))\n",
    "    W2 = tf.Variable(initializer(shape=[12, 25]))\n",
    "    b2 = tf.Variable(initializer(shape=[12, 1]))\n",
    "    W3 = tf.Variable(initializer(shape=[6, 12]))\n",
    "    b3 = tf.Variable(initializer(shape=[6, 1]))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908bcce6",
   "metadata": {},
   "source": [
    "#### Implement forward propagation in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02298e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.math.add(tf.linalg.matmul(W1, X), b1)\n",
    "    A1 = tf.keras.activations.relu(Z1)\n",
    "    Z2 = tf.math.add(tf.linalg.matmul(W2, A1), b2)\n",
    "    A2 = tf.keras.activations.relu(Z2)\n",
    "    Z3 = tf.math.add(tf.linalg.matmul(W3, A2), b3)\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982f1d5",
   "metadata": {},
   "source": [
    "#### Compute the total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac2a2893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss(logits, labels):\n",
    "    logits = tf.transpose(logits)\n",
    "    labels = tf.transpose(labels)\n",
    "    \n",
    "    total_loss = tf.reduce_sum(tf.keras.losses.categorical_crossentropy(y_true=labels, y_pred=logits, from_logits=True))\n",
    "    \n",
    "    return total_loss                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad29d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "\n",
    "    \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    # Initialize your parameters\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    \n",
    "    # The CategoricalAccuracy will track the accuracy for this multiclass problem\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((X_train, Y_train))\n",
    "    test_dataset = tf.data.Dataset.zip((X_test, Y_test))\n",
    "    \n",
    "    # We can get the number of elements of a dataset using the cardinality method\n",
    "    m = dataset.cardinality().numpy()\n",
    "    \n",
    "    minibatches = dataset.batch(minibatch_size).prefetch(8)\n",
    "    test_minibatches = test_dataset.batch(minibatch_size).prefetch(8)\n",
    "    #X_train = X_train.batch(minibatch_size, drop_remainder=True).prefetch(8)# <<< extra step    \n",
    "    #Y_train = Y_train.batch(minibatch_size, drop_remainder=True).prefetch(8) # loads memory faster \n",
    "\n",
    "    # Do the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_total_loss = 0.\n",
    "        \n",
    "        #We need to reset object to start measuring from 0 the accuracy each epoch\n",
    "        train_accuracy.reset_states()\n",
    "        \n",
    "        for (minibatch_X, minibatch_Y) in minibatches:\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # 1. predict\n",
    "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n",
    "\n",
    "                # 2. loss\n",
    "                minibatch_total_loss = compute_total_loss(Z3, tf.transpose(minibatch_Y))\n",
    "\n",
    "            # We accumulate the accuracy of all the batches\n",
    "            train_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
    "            \n",
    "            trainable_variables = [W1, b1, W2, b2, W3, b3]\n",
    "            grads = tape.gradient(minibatch_total_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "            epoch_total_loss += minibatch_total_loss\n",
    "        \n",
    "        # We divide the epoch total loss over the number of samples\n",
    "        epoch_total_loss /= m\n",
    "\n",
    "        # Print the cost every 10 epochs\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_total_loss))\n",
    "            print(\"Train accuracy:\", train_accuracy.result())\n",
    "            \n",
    "            # We evaluate the test set every 10 epochs to avoid computational overhead\n",
    "            for (minibatch_X, minibatch_Y) in test_minibatches:\n",
    "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n",
    "                test_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
    "            print(\"Test_accuracy:\", test_accuracy.result())\n",
    "\n",
    "            costs.append(epoch_total_loss)\n",
    "            train_acc.append(train_accuracy.result())\n",
    "            test_acc.append(test_accuracy.result())\n",
    "            test_accuracy.reset_states()\n",
    "\n",
    "\n",
    "    return parameters, costs, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35214d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
